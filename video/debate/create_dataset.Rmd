---
title: "Create ad-hoc dataset for mexca evaluation"
author: "Eva Viviani"
date: "2022-10-07"
output:
  pdf_document: default
  html_document: default
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)
```

```{r seth path, echo=FALSE}
annotation_folder <- file.path("..","..","data")
output_folder <- file.path("..","..","datasets")
```

### Read camera_shots_coding.csv

The file 'camera_shots_coding.csv' contains the annotation of the faces displayed in the video 'ded21_video.mp4'. The video is 1:44:06 min long. 25 fps. It has been cut in shots.

```{r read annotations, echo=TRUE}
camera_shots_annotation <- read.csv(file.path(annotation_folder, "camera_shots_coding.csv"), 
                                    header = T, 
                                    sep = ";", 
                                    colClasses = c("integer","character","character",
                                                   "factor","factor","factor","factor",
                                                   "NULL"),
                                    na.strings = '')
camera_shots_annotation <- dplyr::slice(camera_shots_annotation, 1:(dplyr::n() -1 )) # bottom row is completely NA -- I  remove it to match the real size of the df
```

```{r head of camera shots, echo=TRUE}
head(camera_shots_annotation)
```

- **shot_id**: integer column that goes from 1 to 1054. Identify the video shot (group of frames)
- **filename**: character column linking the shot_id to its name
- **begin_frame**: time in hours/minutes/seconds/subseconds of the beginning of the frame
- **face_1/2/3**: factor column with the names of face displayed (if known)
- **notes**: factor column containing a string describing of what was displayed when it was something/someone else different from face_1/2/3. It could take the following values: intro, film, groupshot, audience, faces in background

In addition to knowing how many people are displayed, we also have the column `begin_frame` which links the shots to the exact point in time in which they start. 

Load custom functions
```{r}
source("utils.R")
```


### Reshape `camera_shots_coding.csv` 

I found useful to add the columns `duration` and `end_frame` to the dataframe to manually check that the shots were precise. In order to add those columns, I had to parse the begin_frame column, and compute the duration of each shot.

```{r}
camera_shots_annotation$b_frame <- parse_time(camera_shots_annotation)
```

```{r add end_frame column}
camera_shots_annotation$duration <- compute_time_duration(t1 = camera_shots_annotation[, c("b_frame")])
camera_shots_annotation$end_frame <- camera_shots_annotation$b_frame + camera_shots_annotation$duration
```

Another thing that I found useful to balance the dataset was to add a `frame` column whereby I will store the unique frame identifier of that video. This will be used later on to decide if we want a balanced or unbalanced dataset (i.e., having all faces displayed the same n of frames) and how long the clip must be.

```{r}
list_shots <- read.table("list_shots.txt", header = F)
list_shots$V2<- gtools::mixedsort(list_shots$V1) # re-order the file names based on the last number (to match list_filenames)
list_filenames <- unique(camera_shots_annotation$filename)

# how many frames in total across shots
frames <- vector(mode = 'numeric', length = nrow(list_shots))
for(x in 1:nrow(list_shots)){
  frames[x] <- av::av_video_info(list_shots$V2[x])$video[1, c("frames")]
}
# sum(frames)
# [1] 155889

camera_shots_annotation$frames <- frames; rm(frames)

```

#### Count by shot_id how many faces are present, and flag in separate columns what type of faces are

To be able to select the shots based on number of speakers, we need to count how many faces are displayed per shot_id in the face_1/2/3 columns. Note that shot_id equals to the number of rows, thus the use of `rowSums()`.

```{r count speakers by row, echo=TRUE}

camera_shots_annotation$n_faces <- rowSums(!is.na(camera_shots_annotation[,c("face_1", "face_2", "face_3")]))
```

Recall that face_1/2/3 contain a value if one of the faces displayed are known. However, there are instances in which in addition/instead to the face known, there are also other faces (e.g., audience) which will be picked up by the model. We want to be able to filter out those situations by flagging their presence in specific columns. In addition, we want to flag instances where the whole group of known faces was displayed (i.e., groupshot).
Note that 'faces in background' are always in addition to what has been noted in the other columns, and not mutually exclusive.

```{r unpack notes column, echo=TRUE}

camera_shots_annotation <- camera_shots_annotation |>
  dplyr::mutate(groupshot = ifelse(notes == "groupshot", 1, 0),
                faces_in_background = ifelse(notes == "faces in background", 1, 0),
                audience = ifelse(notes == "audience", 1, 0),
                film = ifelse(notes == "film", 1,0),
                intro = ifelse(notes == "intro", 1,0))

```

This is our final dataframe: 
```{r}
head(camera_shots_annotation)
```

### Select shots based on number of speakers selected and assemble a clip

From the annotation (i.e., `camera_shots_annotation`) we now create multiple clips. We will run mexca on all those clips and with the data obtained we can answer two questions:

1) "how many speakers the model can recognise when there are X speakers present at the same time?" (i.e., when they are displayed together) -- contemporaries = T

2) "how many speakers the model can recognise when there are X speakers in total?" (i.e., across the whole clip)  -- contemporaries = F


We start from (1).
Note: For (1), we know from the `camera_shots_coding.csv` that the max number of known speakers that are displayed at the same time is 3.

This function below with `contemporaries = T` creates datasets for answering Q1, with number_of_speakers set to 3.
```{r Q1, eval=FALSE}
make_clip(number_of_speakers = seq_len(3), camera_shots_annotation, contemporaries = T, n_unique_combination = 1, max_shots = 2)
```

Function below answers Q2. However, it does by concatenating clips of 1 person, up to n_unique_combination. These unique combinations however are matched in terms of how long each face/speaker is displayed.
```{r Q2, eval=FALSE}
make_clip(number_of_speakers = 1, camera_shots_annotation, contemporaries = F, n_unique_combination = 2:10, max_shots = 2) #note that starting from 1 makes a dataset equivalent to Q1

```

Both functions save `.txt` and `.mp4`. The latter ready to be processed by mexca.
