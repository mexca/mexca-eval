{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run mexca's video pipeline on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mexca.video.extraction_develop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Video\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmexca\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvideo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextraction_develop\u001b[39;00m \u001b[39mimport\u001b[39;00m FaceExtractor\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mexca.video.extraction_develop'"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import Video\n",
    "from mexca.video.extraction_develop import FaceExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class FaceExtractor:\n",
      "    \"\"\"Combine steps to extract features from faces in a video file.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    au_model: {'JAANET', 'svm', 'logistic'}\n",
      "        The name of the pretrained model for detecting action units. Default is `JAANET`.\n",
      "    landmark_model: {'PFLD', 'MobileFaceNet', 'MobileNet'}\n",
      "        The name of the pretrained model for detecting facial landmarks. Default is `PFLD`.\n",
      "    **clargs: dict, optional\n",
      "        Additional arguments that are passed to the ``spectralcluster.SpectralClusterer`` class instance.\n",
      "\n",
      "    Attributes\n",
      "    ----------\n",
      "    mtcnn\n",
      "    resnet\n",
      "    cluster\n",
      "    pyfeat\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "    For details on the available `au_model` and `landmark_model` arguments,\n",
      "    see the documentation of `py-feat <https://py-feat.org/pages/models.html>`_.\n",
      "    The pretrained action unit models return different outputs: `JAANET` returns intensities (0-1) for 12 action units,\n",
      "    whereas `svm` and `logistic` return presence/absence (1/0) values for 20 action units.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "    def __init__(self, au_model='JAANET', landmark_model='PFLD', **clargs) -> 'FaceExtractor':\n",
      "        self.mtcnn = MTCNN(keep_all=True)\n",
      "        self.resnet = InceptionResnetV1(\n",
      "            pretrained='vggface2'\n",
      "        ).eval()\n",
      "        self.cluster = SpectralClusterer(**clargs)\n",
      "        self.pyfeat = feat.detector.Detector(\n",
      "            au_model=au_model,\n",
      "            landmark_model=landmark_model\n",
      "        )\n",
      "\n",
      "\n",
      "    @property\n",
      "    def mtcnn(self):\n",
      "        \"\"\"The MTCNN model for face detection and extraction.\n",
      "        Must be instance of ``MTCNN`` class.\n",
      "        See `facenet-pytorch <https://github.com/timesler/facenet-pytorch>`_ for details.\n",
      "        \"\"\"\n",
      "        return self._mtcnn\n",
      "\n",
      "\n",
      "    @mtcnn.setter\n",
      "    def mtcnn(self, new_mtcnn):\n",
      "        if isinstance(new_mtcnn, MTCNN):\n",
      "            self._mtcnn = new_mtcnn\n",
      "        else:\n",
      "            raise TypeError('Can only set \"mtcnn\" to instances of the \"MTCNN\" class')\n",
      "\n",
      "\n",
      "    @property\n",
      "    def resnet(self):\n",
      "        \"\"\"The ResnetV1 model for computing face embeddings. Uses the pretrained 'vggface2' version by default.\n",
      "        Must be instance of ``InceptionResnetV1`` class.\n",
      "        See `facenet-pytorch <https://github.com/timesler/facenet-pytorch>`_ for details.\n",
      "        \"\"\"\n",
      "        return self._resnet\n",
      "\n",
      "\n",
      "    @resnet.setter\n",
      "    def resnet(self, new_resnet):\n",
      "        if isinstance(new_resnet, InceptionResnetV1):\n",
      "            self._resnet = new_resnet\n",
      "        else:\n",
      "            raise TypeError('Can only set \"resnet\" to instances of the \"InceptionResnetV1\" class')\n",
      "\n",
      "\n",
      "    @property\n",
      "    def cluster(self):\n",
      "        \"\"\"The spectral clustering model for identifying faces based on embeddings.\n",
      "        Must be instance of ``SpectralClusterer`` class.\n",
      "        See `spectralcluster <https://wq2012.github.io/SpectralCluster/>`_ for details.\n",
      "        \"\"\"\n",
      "        return self._cluster\n",
      "\n",
      "\n",
      "    @cluster.setter\n",
      "    def cluster(self, new_cluster):\n",
      "        if isinstance(new_cluster, SpectralClusterer):\n",
      "            self._cluster = new_cluster\n",
      "        else:\n",
      "            raise TypeError('Can only set \"cluster\" to instances of the \"SpectralClusterer\" class')\n",
      "\n",
      "\n",
      "    @property\n",
      "    def pyfeat(self):\n",
      "        \"\"\"The model for extracting facial landmarks and action units. Must be instance of ``Detector`` class.\n",
      "        See `py-feat <https://py-feat.org/pages/api.html>`_ for details.\n",
      "        \"\"\"\n",
      "        return self._pyfeat\n",
      "\n",
      "\n",
      "    @pyfeat.setter\n",
      "    def pyfeat(self, new_pyfeat):\n",
      "        if isinstance(new_pyfeat, feat.detector.Detector):\n",
      "            self._pyfeat = new_pyfeat\n",
      "        else:\n",
      "            raise TypeError('Can only set \"pyfeat\" to instances of the \"Detector\" class')\n",
      "\n",
      "\n",
      "    def detect(self, frame):\n",
      "        \"\"\"Detect faces in an image array.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        frame: numpy.ndarray\n",
      "            Array containing the RGB values of a video frame with dimensions (H, W, 3).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        faces: torch.tensor\n",
      "            Tensor containing the N cropped face images from the frame with dimensions (N, 3, 160, 160).\n",
      "        boxes: numpy.ndarray\n",
      "            Array containing the bounding boxes of the N detected faces as (x1, y1, x2, y2) coordinates with\n",
      "            dimensions (N, 4).\n",
      "        probs: numpy.ndarray\n",
      "            Probabilities of the detected faces.\n",
      "\n",
      "        \"\"\"\n",
      "\n",
      "        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
      "\n",
      "        boxes, probs = self.mtcnn.detect(img, landmarks=False) # pylint: disable=unbalanced-tuple-unpacking\n",
      "\n",
      "        faces = self.mtcnn.extract(frame, boxes, save_path=None)\n",
      "\n",
      "        return faces, boxes, probs\n",
      "\n",
      "\n",
      "    def encode(self, faces):\n",
      "        \"\"\"Compute embeddings for face images.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        faces: torch.tensor\n",
      "            Tensor containing N face images with dimensions (N, 3, H, W). H and W must at least be 80 for\n",
      "            the encoding to work.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Array containing embeddings of the N face images with dimensions (N, 512).\n",
      "\n",
      "        \"\"\"\n",
      "        embeddings = self.resnet(faces).numpy()\n",
      "\n",
      "        return embeddings\n",
      "\n",
      "\n",
      "    def identify(self, embeddings):\n",
      "        \"\"\"Cluster faces based on their embeddings.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        embeddings: numpy.ndarray\n",
      "            Array containing embeddings of the N face images with dimensions (N, E) where E is the length\n",
      "            of the embedding vector.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        numpy.ndarray\n",
      "            Cluster indices for the N face embeddings.\n",
      "\n",
      "        \"\"\"\n",
      "        labels = np.full((embeddings.shape[0]), np.nan)\n",
      "        label_finite = np.all(np.isfinite(embeddings), 1)\n",
      "        labels[label_finite] = self.cluster.predict(\n",
      "            embeddings[label_finite, :])\n",
      "\n",
      "        return labels\n",
      "\n",
      "\n",
      "    def extract(self, frame, boxes):\n",
      "        \"\"\"Detect facial action units and landmarks.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        frame: numpy.ndarray\n",
      "            Array containing the RGB values of a video frame with dimensions (H, W, 3).\n",
      "        boxes: numpy.ndarray\n",
      "            Array containing the bounding boxes of the N detected faces as (x1, y1, x2, y2) coordinates\n",
      "            with dimensions (N, 4).\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        landmarks: numpy.ndarray\n",
      "            Array containg facial landmarks for N detected faces as (x, y) coordinates with dimensions (N, 68, 2).\n",
      "        aus: numpy.ndarray\n",
      "            Array containing action units for N detected faces with dimensions (N, U).\n",
      "            The number of detected actions units U varies across `au_model` specifications.\n",
      "\n",
      "        \"\"\"\n",
      "        if frame.ndim == 3:\n",
      "            frame = np.expand_dims(frame, 0)  # convert to 4d\n",
      "\n",
      "        boxes_list = boxes.reshape(1, -1, 4).tolist()\n",
      "        landmarks = self.pyfeat.detect_landmarks(frame, boxes_list)\n",
      "        if self.pyfeat['au_model'].lower() in ['svm', 'logistic']:\n",
      "            hog, new_landmarks = self.pyfeat._batch_hog(  # pylint: disable=protected-access\n",
      "                frames=frame, detected_faces=boxes_list, landmarks=landmarks\n",
      "            )\n",
      "            aus = self.pyfeat.detect_aus(hog, new_landmarks)\n",
      "        else:\n",
      "            aus = self.pyfeat.detect_aus(frame, landmarks)\n",
      "\n",
      "        # Remove first redundant dimension from landmarks array; new first dim = # of detected faces\n",
      "        landmarks_np = np.array(landmarks).reshape((-1, 68, 2))\n",
      "\n",
      "        return landmarks_np, aus\n",
      "\n",
      "\n",
      "    @staticmethod\n",
      "    def check_skip_frames(skip_frames):\n",
      "        if isinstance(skip_frames, int):\n",
      "            if skip_frames < 1:\n",
      "                raise ValueError('Argument \"skip_frames\" must be >= 1')\n",
      "        else:\n",
      "            raise TypeError('Argument \"skip_frames\" must be int')\n",
      "\n",
      "\n",
      "    def apply(self, filepath, skip_frames=1, process_subclip=(0, None), show_progress=True):  # pylint: disable=too-many-locals\n",
      "        \"\"\"Apply multiple steps to extract features from faces in a video file.\n",
      "\n",
      "        This method subsequently calls other methods for each frame of a video file to detect and cluster faces.\n",
      "        It also extracts the facial landmarks and action units.\n",
      "\n",
      "        Parameters\n",
      "        ----------\n",
      "        filepath: str or path\n",
      "            Path to the video file.\n",
      "        skip_frames: int, default=1\n",
      "            Forces extractor to only process every nth frame.\n",
      "        process_subclip: tuple, default=(0, None)\n",
      "            Process only a part of the video clip.\n",
      "            See `moviepy.editor.VideoFileClip\n",
      "            <https://moviepy.readthedocs.io/en/latest/ref/VideoClip/VideoClip.html#videofileclip>`_ for details.\n",
      "        show_progress: bool, default=True\n",
      "            Enables the display of a progress bar.\n",
      "\n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "            A dictionary with extracted facial features.\n",
      "\n",
      "        \"\"\"\n",
      "        if not isinstance(show_progress, bool):\n",
      "            raise TypeError('Argument \"show_progress\" must be bool')\n",
      "\n",
      "        self.check_skip_frames(skip_frames)\n",
      "\n",
      "        with VideoFileClip(filepath, audio=False, verbose=False) as clip:\n",
      "            subclip = clip.subclip(process_subclip[0], process_subclip[1])\n",
      "            features = {\n",
      "                'frame': [],\n",
      "                'time': [],\n",
      "                'face_box': [],\n",
      "                'face_prob': [],\n",
      "                'face_landmarks': [],\n",
      "                'face_aus': []\n",
      "            }\n",
      "\n",
      "            embeddings = []  # Embeddings are separate because they won't be returned\n",
      "\n",
      "            n_frames = int(subclip.duration*subclip.fps)\n",
      "\n",
      "            if skip_frames > n_frames:\n",
      "                raise SkipFramesError('Arguments \"skip_frames\" cannot be higher than the total frames in the video')\n",
      "\n",
      "            for i, (t, frame) in tqdm(\n",
      "                enumerate(subclip.iter_frames(with_times=True)),\n",
      "                total=n_frames,\n",
      "                disable=not show_progress\n",
      "            ):\n",
      "                if i % skip_frames == 0:\n",
      "\n",
      "                    faces, boxes, probs = self.detect(frame)\n",
      "\n",
      "                    if faces is None:\n",
      "                        features['frame'].append(i)\n",
      "                        features['time'].append(t)\n",
      "                        features['face_box'].append(np.nan)\n",
      "                        features['face_prob'].append(np.nan)\n",
      "                        features['face_landmarks'].append(np.nan)\n",
      "                        features['face_aus'].append(np.nan)\n",
      "\n",
      "                        embeddings.append(\n",
      "                            np.full((self.resnet.last_bn.num_features), np.nan))\n",
      "                    else:\n",
      "                        embs = self.encode(faces)  # Embeddings per frame\n",
      "                        landmarks, aus = self.extract(frame, boxes)\n",
      "\n",
      "                        for box, prob, emb, landmark, au in zip(boxes, probs, embs, landmarks, aus):\n",
      "                            features['frame'].append(i)\n",
      "                            features['time'].append(t)\n",
      "                            features['face_box'].append(box)\n",
      "                            features['face_prob'].append(prob)\n",
      "                            features['face_landmarks'].append(landmark)\n",
      "                            features['face_aus'].append(au)\n",
      "\n",
      "                            embeddings.append(emb)\n",
      "\n",
      "            features['face_id'] = self.identify(\n",
      "                np.array(embeddings).squeeze()).tolist()\n",
      "\n",
      "            return features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "\n",
    "fun_df = inspect.getsource(FaceExtractor)\n",
    "print(fun_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing video @ /Users/evaviviani/github/mexca-eval/video/debate/total/2_speaker_total.mp4\n",
      " ...\n",
      "Using 2 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "606it [04:24,  2.29it/s]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result saved @ 2_speaker_total.csv\n",
      "Computing video @ /Users/evaviviani/github/mexca-eval/video/debate/total/3_speaker_total.mp4\n",
      " ...\n",
      "Using 3 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 118/982 [00:56<06:17,  2.29it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "f = open('list_clips_.txt','r')\n",
    "for filepath in f.readlines():\n",
    "    n_clusters = int(filepath.strip().split(\"/\")[-1].split('_')[0])\n",
    "\n",
    "    video = FaceExtractor(\n",
    "        au_model = 'JAANET',\n",
    "        landmark_model='PFLD',\n",
    "        min_clusters = n_clusters,\n",
    "        max_clusters = n_clusters\n",
    "    )\n",
    "    print(f'Computing video @ {filepath} ...')\n",
    "    print(f'Using {n_clusters} clusters')\n",
    "    output = video.apply(filepath.strip())\n",
    "    output_df = pd.DataFrame(output)\n",
    "    output_fname = filepath.split('/')[-1].split('.')[0]+'.csv'\n",
    "    output_df.to_csv(output_fname,index=False)\n",
    "    print(f'result saved @ {output_fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mexca_october_22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "001e1c0fa8f3df81934e2d8f25d5607606c89bdb27a35bc13df9b3192171a0f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
